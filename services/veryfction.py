import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from tasks.models import Task


def url_form_sitemap_html(sitemap_url, keyword):
    task_review = ['ØªÙ‚ÙŠÙŠÙ…', 'ØªØ­Ø¯ÙŠØ«/ØªÙ‚ÙŠÙŠÙ…']

    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        read_sitemap = requests.get(sitemap_url, headers=headers, timeout=10)
        print("Status:", read_sitemap.status_code)
        if read_sitemap.status_code != 200:
            print("âŒ Ø§Ù„ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£Ùˆ Ù‡Ù†Ø§Ùƒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„.")
            return []

        soup = BeautifulSoup(read_sitemap.text, "html.parser")

        # =========================
        # Ù…Ø¹Ø§Ù„Ø¬Ø© keyword Ù„Ùˆ ÙƒØ§Ù† Ø±Ø§Ø¨Ø·
        # =========================
        if keyword.startswith("http"):
            parsed = urlparse(keyword)
            keyword = parsed.netloc or parsed.path
            keyword = keyword.replace("www.", "")

            for ext in [".com", ".net", ".org", ".io", ".co", ".ae", ".sa", "ØªÙ‚ÙŠÙŠÙ…", "Ø´Ø±ÙƒØ©", "Ø§ÙØ¶Ù„ Ø´Ø±ÙƒØ§Øª",'/ar']:
                keyword = keyword.replace(ext, "")

        keyword_words = keyword.lower().split()
        min_match = 2 if len(keyword_words) > 2 else 1

        
        task = Task.objects.filter(article_title=keyword).first()
        if task and task.article_type_W_R_A_B in task_review:
            min_match = 1 if len(keyword_words) > 2 else 1

        reslist = []
        seen_links = set()

        for link in soup.find_all("a"):
            anchor_text = link.get_text(strip=True) or ""
            href = link.get("href")
            if not href:
                continue

            full_href = urljoin(sitemap_url, href)

            if full_href in seen_links:
                continue
            seen_links.add(full_href)

            anchor_words = anchor_text.lower().split()
            match_count = sum(1 for word in keyword_words if word in anchor_words)

            if match_count >= min_match:
                reslist.append((full_href, anchor_text, match_count))

        reslist.sort(key=lambda x: x[2], reverse=True)
        return reslist

    except requests.RequestException as e:
        print("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø©.", str(e))
        return []


# Ù…Ø«Ø§Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…
sitemap_url = "https://daman.reviews/sitemap"
keyword = "Ø§Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© ØªÙ‚ÙŠÙŠÙ… Ø´Ø±ÙƒØ© fxcc  Ø¨Ø´ÙƒÙ„ Ø§ÙŠØ¬Ø§Ø¨ÙŠ"

found_links = url_form_sitemap_html(sitemap_url, keyword)

print("\nğŸ” Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©ØŒ Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø±):")
if found_links:
    for href, text, count in found_links:
        print(f"Ø§Ù„Ù†Øµ: {text}\nØ§Ù„Ø±Ø§Ø¨Ø·: {href}\nØ¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©: {count}\n---")
else:
    print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø±ÙˆØ§Ø¨Ø· ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø¬Ù…Ù„Ø©.")
